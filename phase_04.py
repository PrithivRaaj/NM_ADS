# -*- coding: utf-8 -*-
"""Phase-04

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CNW0tkU5vr6sF9Dmdx4UEo9gQls9w0h2
"""

# Import all the necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

# Read the dataset
df = pd.read_csv("Sales.csv")

# Display the first ten rows
df.head(10)

# Display the last rows
df.tail()

# Describe the dataset
df.describe()

# Get the shape of the dataset
df.shape

# Get the size of the dataset
df.size

# Get information about the dataset
df.info()

# Check for null values
df.isnull()

# Total null values
df.isnull().sum().sum()

# Get the columns
df.columns

# Visualize outliers using box plots
fig, axis = plt.subplots(3, figsize=(5, 5))
plt1 = sns.boxplot(df['TV'], ax=axis[0])
plt2 = sns.boxplot(df['Radio'], ax=axis[1])
plt3 = sns.boxplot(df['Newspaper'], ax=axis[2])
plt.tight_layout()

# Assign variables for feature engineering
x1 = df["TV"]
x2 = df["Newspaper"]
x3 = df["Radio"]
y = df["Sales"]

# Plotting the original features
plt.scatter(x1, y)
plt.scatter(x2, y)
plt.scatter(x3, y)

# Standardize the features using StandardScaler
scaler = StandardScaler()
x1 = np.array(x1).reshape(-1, 1)
x2 = np.array(x2).reshape(-1, 1)
scaler_x1 = StandardScaler().fit(x1)
scaler_x2 = StandardScaler().fit(x2)
x1_scaled = scaler_x1.transform(x1)
x2_scaled = scaler_x2.transform(x2)



# Feature Engineering
# 1. Interaction Features
df['TV_Radio'] = df['TV'] * df['Radio']
df['TV_Newspaper'] = df['TV'] * df['Newspaper']
df['Radio_Newspaper'] = df['Radio'] * df['Newspaper']

# 2. Polynomial Features (for example, quadratic features)
df['TV^2'] = df['TV']**2
df['Radio^2'] = df['Radio']**2
df['Newspaper^2'] = df['Newspaper']**2

# 3. Log Transformation (if the distribution is skewed)
df['TV_log'] = np.log(df['TV'])
df['Radio_log'] = np.log(df['Radio'])
df['Newspaper_log'] = np.log(df['Newspaper'])

#linear regression
#importing linear regression library
from sklearn.linear_model import LinearRegression
#train test split
from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X = np.column_stack((x1, x2, x3))  # Use the features you want to include
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and fit a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the model's performance metrics
print("Mean Squared Error:", mse)
print("R-squared (R2) Score:", r2)

#the model's coefficients and intercept
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)